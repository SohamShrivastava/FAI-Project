üêç Reinforcement Learning Snake Game

A complete reinforcement-learning project implementing Random, SARSA, and Deep Q-Network (DQN) agents to play the classic Snake game.
Built using Python, PyTorch, and Pygame.

üöÄ Overview

This repository contains a modular RL framework for training different agents on a custom Snake environment.
It includes:

‚úî Custom Snake environment with Pygame

‚úî Three agent types: Random, SARSA, DQN

‚úî Replay memory + batch training for DQN

‚úî Neural network model with checkpoint saving

‚úî Clear code structure & extensible design

üìÇ Project Structure
‚îú‚îÄ‚îÄ agents.py         
‚îú‚îÄ‚îÄ environment.py   
‚îú‚îÄ‚îÄ neural_net.py     
‚îú‚îÄ‚îÄ run_experiment.py           
‚îî‚îÄ‚îÄ README.md

üéÆ Snake Environment (environment.py)
State Representation (size = 11)

Your agent receives an 11-dimensional binary feature vector:

Danger straight

Danger right

Danger left

Moving left

Moving right

Moving up

Moving down

Food left

Food right

Food up

Food down

Action Space (one-hot encoding)
[1, 0, 0] ‚Üí move straight  
[0, 1, 0] ‚Üí turn right  
[0, 0, 1] ‚Üí turn left

Reward Function
Event	Reward
Eating food	+10
Dying (collision or wall)	‚Äì10
Normal step	0
Game End Conditions

Snake hits wall

Snake hits its own body

Too many steps without eating (frame_iteration > 100 √ó length)

üß† RL Agents (agent.py)
RandomAgent

Baseline model that picks random moves.

SARSA Agent

Tabular SARSA(0):

Œµ-greedy exploration

Q-table dictionary (state ‚Üí [Q(a‚ÇÄ), Q(a‚ÇÅ), Q(a‚ÇÇ)])

Online update rule

DQN Agent

Deep Q-learning with:

Replay Memory: 100,000

Batch Size: 1,000

Œ≥ = 0.9

Adam optimizer (lr = 0.001)

MSE loss

Two-layer neural network (11 ‚Üí 512 ‚Üí 3)

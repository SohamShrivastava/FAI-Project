# ğŸ Reinforcement Learning Snake Game

A complete reinforcement-learning project implementing Random, SARSA, and Deep Q-Network (DQN) agents to play the classic Snake game.
Built using Python, PyTorch, and Pygame.

---

## ğŸš€ Overview

This repository contains a modular RL framework for training different agents on a custom Snake environment.

### âœ” Features

- Custom Snake environment using **Pygame**
- Three agent types: **Random**, **SARSA**, **DQN**
- Replay memory + batch training for DQN
- Neural network with checkpoint saving
- Clean, extensible code structure

---

## ğŸ“‚ Project Structure

â”œâ”€â”€ agents.py         
â”œâ”€â”€ environment.py   
â”œâ”€â”€ neural_net.py     
â”œâ”€â”€ run_experiment.py           
â””â”€â”€ README.md

---

## ğŸ® Snake Environment (`environment.py`)

### ğŸ“Œ **State Representation (11 Features)**

Each observation is encoded as an **11-dimensional binary feature vector**:

#### ğŸ›‘ Danger Awareness
1. Danger straight  
2. Danger right  
3. Danger left  

#### ğŸ§­ Current Movement Direction
4. Moving left  
5. Moving right  
6. Moving up  
7. Moving down  

#### ğŸ Food Position (Relative to Head)
8. Food left  
9. Food right  
10. Food up  
11. Food down  

---

### ğŸ’€ **Game Termination Conditions**

An episode ends when:

- ğŸ§± Snake hits the wall  
- ğŸŒ€ Snake collides with its own body  
- â³ Too many steps without eating (starvation)


---

## ğŸ§  Reinforcement Learning Agents (`agents.py`)

### ğŸ² **RandomAgent**
A simple baseline agent that:
- Chooses actions uniformly at random  
- Provides a reference for measuring RL improvements  

---

### ğŸ“˜ **SARSA Agent â€“ Tabular SARSA(0)**

Implements **on-policy Temporal Difference learning**.

#### Key Features:
- Îµ-greedy action selection (Îµ decays over time)
- Q-table stored as:  
  **state â†’ [Q(aâ‚€), Q(aâ‚), Q(aâ‚‚)]**
- Online update rule after each step

---

### ğŸ¤– **DQN Agent â€“ Deep Q-Learning**

A neural networkâ€“based agent capable of learning complex strategies.

#### ğŸ”§ Architecture:
- Input â†’ Hidden (512 ReLU) â†’ Output  
  **11 â†’ 512 â†’ 3**
